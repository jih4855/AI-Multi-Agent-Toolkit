<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.8/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-sRIl4kxILFvY47J16cr9ZwB07vP4J8+LH7qKQnuqkuIAvNWLzeN8tE5YBujZqJLB" crossorigin="anonymous">
    <link rel="stylesheet" href="styles.css?v=2">
    <!-- CSS가 별도 파일로 분리되었습니다 -->

    <title>AI Multi-Agent Toolkit - Documentation</title>
    <script src="scripts.js?v=2"></script>
</head>
<body>
    <nav class="navbar navbar-expand-lg sticky-top" data-bs-theme="dark">
        <div class="container-fluid">
            <a class="navbar-brand" href="#">AI Multi-Agent Toolkit</a>
        </div>
    </nav>

    <!-- 레이아웃 래퍼 추가 -->
    <div class="layout">
        <nav class="sidebar">
            <div class="sidebar-title">AI Toolkit</div>
            <ul id="sidebar-nav"></ul>
        </nav>

        <main class="content">
            <!-- 기존 문서 컨텐츠 래핑 -->
            <div class="docs-section">
                <h3>1.업데이트 현황</h3>
                <summary>현재까지 구현된 기능에 대한 요약입니다.</summary>
                <ul>
                    <li>LLM 에이전트 생성 및 응답 받기 (ollama, openai, genai(gemini) 지원)</li>
                    <li>Discord로 메시지 보내기 (메시지 청크 분할 및 겹침 지원)</li>
                    <li>음성 파일을 텍스트로 변환하기 (whisper 사용)</li>
                    <li>텍스트 청크 분할 및 겹침 지원</li>
                    <li>2025/09/17 에이전트 메모리 기능 구현</li>
                    <li>2025/09/21 LLM_Agent 클래스에 __call__ 매직 메서드가 추가되어 agent.generate_response() 대신 agent() 직접 호출이 가능해졌습니다.</li>
                </ul>
                <p>추가 기능은 추후 업데이트 예정입니다.</p>

                <h3>2. LLM 에이전트</h3>
                <details>
                    <summary>llm에이전트 생성하기</summary>
                    <p>다음과 같이 에이전트를 생성하세요.</p>
                    <div class="code-container">
                        <button class="copy_button" onclick="copy_button('copy1')">Copy</button>
                        <pre><code id="copy1">
<var>model_name</var> = 'gemma:12b' <span class="comment">#사용할 모델명을 입력하세요</span>
<var>system_prompt</var> = '당신은 유능한 비서입니다. 이용자에게 도움이 되는 답변을 제공합니다.'
<var>user_prompt</var> = '프랑스의 수도는 어디인가요?'
<var>provider</var> = 'ollama'  <span class="comment">#현재 사용가능한 provier는 "ollama", "openai","genai(gemini)"입니다</span>

<var>agent</var> = LLM_Agent(model_name, provider, api_key=None)

<var>response</var> = agent(system_prompt, user_prompt, task=None)
print(response)
                        </code></pre>
                    </div>
                    <p>위 코드에서 <code>model_name</code>은 사용할 모델의 이름을 지정합니다. <code>system_prompt</code>는 모델에게 주어지는 시스템 메시지이므로 역할 및 페르소나를 지정하세요. <code>user_prompt</code>는 llm에게 질문 및 작업을 요청하세요. <code>provider</code>는 사용할 LLM 제공자를 지정합니다.</p>
                    <p><code>task</code>의 기본값은 None이고 필요시 추가 하세요.</p>
                    <p><code>api_key</code>의 기본값을 None이므로, ollama를 사용할 경우 API 키가 필요하지 않습니다.</p>
                </details>

                <details>
                    <summary>멀티 에이전트 활용하기</summary>
                    <p>멀티 에이전트를 활용하려면 다음과 같이 하세요.</p>
                    <p>여러개의 LLM 에이전트를 구성하세요. 순차적으로 에이전트를 호출하고, 앞의 에이전트의 응답을 다음 에이전트의 프롬프트에 포함시킵니다.</p>
                    <div class="code-container">
                        <button class="copy_button" onclick="copy_button('copy1-1')">Copy</button>
                        <pre><code id="copy1-1">
from module.llm_agent import LLM_Agent

<var>system_prompt</var> = "You are a helpful assistant."
<var>agent1_user_prompt</var> = "리눅스에 대해서 설명해주세요."
<var>agent2_user_prompt</var> = "앞선 답변을 읽고 내용을 보충해 주세요"
<span class="comment"># 복수의 에이전트의 프롬프트를 정의합니다.</span>
<var>multi_agent</var> = LLM_Agent(model_name="gemma3n", provider="ollama")
<var>agent1</var> = multi_agent(system_prompt, agent1_user_prompt, task=None)
<var>agent2</var> = multi_agent(system_prompt, agent2_user_prompt, task=None, multi_agent_response=agent1)
<span class="comment"># agent1의 답변을 이어 받아, agent2의 프롬프트에 포함시킵니다.</span>
print("Agent 1 Response:", agent1)
print("Agent 2 Response:", agent2)
                        </code></pre>
                    </div>
                    <p>위 코드에서 <code>multi_agent_response</code> 매개변수를 사용하여 이전 에이전트의 응답을 다음 에이전트에게 전달합니다. 작업을 분할하고 각 에이전트에 맞게 프롬프트를 조정할 수 있습니다.</p>
                    <p>첫번째 에이전트의 작업을 두번째 에이전트에 전달하여, 각 에이전트가 <var>task</var>를 심화하여 수행할 수 있도록 설정하세요.</p>
                </details>

                <details>
                    <summary>모든 에이전트의 응답 통합하기</summary>
                    <p>에이전트에게 작업을 각각 할당하고 통합하여 최종 응답을 생성합니다.</p>
                    <div class="code-container">
                        <button class="copy_button" onclick="copy_button('copy1-2')">Copy</button>
                        <pre><code id="copy1-2">
from module.llm_agent import LLM_Agent

<span class="comment">#각 에이전트의 시스템 프롬프트, 사용자 프롬프트, 작업을 정의합니다.</span>
<var>multi_agent_tasks</var> = {
    "Agent 1": "도시에서 발생하는 환경 문제(대기, 수질, 쓰레기 등)를 정리하고, 가장 시급한 과제를 제시한다.",
    "Agent 2": "친환경 교통수단(대중교통, 자전거, 전기차 등)을 기반으로 지속 가능한 교통 인프라 계획을 제안한다.",
    "Agent 3": "재생에너지(태양광, 풍력, 스마트 그리드 등)를 활용하여 효율적인 에너지 공급 방안을 설계한다.",
    "Agent 4": "도시 공간 구조(공원, 주거, 상업지구 배치 등)를 최적화한다."
}
<var>multi_agent_system_prompts</var> = {
    "Agent 1": "당신은 환경 전문가입니다. 도시의 환경 문제를 분석하고, 가장 시급한 문제를 제시하세요.",
    "Agent 2": "당신은 교통 전문가입니다. 지속 가능한 교통 인프라 계획을 제안하세요.",
    "Agent 3": "당신은 에너지 전문가입니다. 재생에너지를 활용한 에너지 공급 방안을 설계하세요.",
    "Agent 4": "당신은 도시 계획 전문가입니다. 도시 공간 구조를 최적화하는 방안을 제시하세요."
}
<var>user_prompts</var> ={
    "Agent 1": "도시에서 발생하는 환경 문제를 분석하고, 가장 시급한 문제를 제시하세요.",
    "Agent 2": "지속 가능한 교통 인프라 계획을 제안하세요.",
    "Agent 3": "재생에너지를 활용한 에너지 공급 방안을 설계하세요.",
    "Agent 4": "도시 공간 구조를 최적화하는 방안을 제시하세요."
}


<var>order</var> = ["Agent 1", "Agent 2", "Agent 3", "Agent 4"]

<var>multi_agent</var> = LLM_Agent(model_name="gemini-2.5-flash", provider="genai", api_key="your_api_key") <span class="comment">#여러 에이전트를 생성합니다.</span>

<var>agent_responses</var> = {
    name: multi_agent(multi_agent_system_prompts[name], user_prompts[name], multi_agent_tasks[name])
    for name in order
}
<var>response_list</var> = [agent_responses[name] for name in order] <span class="comment">#각 에이전트의 응답을 순서대로 리스트에 저장합니다.</span>

<var>multi_agent_responses</var> = multi_agent(
    "당신은 도시 계획 전문가입니다. 지속 가능한 도시 설계 방안을 제시하세요.",
    "다음은 여러 전문가의 의견입니다. 이를 바탕으로 최종 요약 및 통합된 지속 가능한 도시 설계 방안을 제시하세요.",
    "최종 요약 및 통합된 지속 가능한 도시 설계 방안을 제시한다.",
    response_list
) <span class="comment">#모든 에이전트의 응답을 통합하여 최종 응답을 생성합니다.</span>

print("Agent 1 Response:", agent_responses["Agent 1"])
print("Agent 2 Response:", agent_responses["Agent 2"])
print("Agent 3 Response:", agent_responses["Agent 3"])
print("Agent 4 Response:", agent_responses["Agent 4"])
print("Multi-Agent Responses:", multi_agent_responses)
                        </code></pre>
                    </div>
                    <p>위 코드에서 각 에이전트는 특정 작업을 수행하고, 그 응답은 최종 통합 응답을 생성하는 데 사용됩니다. 에이전트 별로 각각의 작업을 할당하고, 하나로 통합한 답을 생성합니다.</p>
                </details>

                <details>
                    <summary>LLM에이전트에 기억력 붙이기</summary>
                    <p>LLM 에이전트에 기억력을 추가하여 이전 대화 내용을 기억하고 활용할 수 있습니다.</p>
                    <div class="code-container">
                        <button class="copy_button" onclick="copy_button('copy1-3')">Copy</button>
                        <pre><code id="copy1-3">
import dotenv
import os
dotenv.load_dotenv()

# LLM_Agent 인스턴스 생성
<var>llm</var> = LLM_Agent(model_name="gemini-2.5-flash", provider="genai", api_key=os.getenv("GENAI_API_KEY"), max_history=10) <span class="comment">#max_history = 기억할 대화 수</span>
# 대화 루프 예시

while True:
    <var>user_input</var> = input("You: ")
    <var>response</var> = llm(system_prompt="You are a helpful assistant.", user_message=user_input, memory=True) <span class="comment">#memory=True로 설정하여 기억력 활성화</span>
    print("Assistant:", response)

    if user_input.lower() in ['exit', 'quit']:
        break
                        </code></pre>
                    </div>
                    <p>간단한 대화 예시입니다. max_history=10으로 설정하여 10개의 이전 대화를 기억합니다.</p>
                    <p>memory를 활성화 시켜 이전 대화 내용을 활용할 수 있습니다. DB = sqlite3로 제작되었습니다.</p>
                </details>

                <h3>3. Discord로 메세지 보내기</h3>
                <details>
                    <summary>LLM응답 결과를 Discord로 전송하기</summary>
                    <p>다음과 같이 Discord로 메시지를 전송할 수 있습니다.</p>
                    <div class="code-container">
                        <button class="copy_button" onclick="copy_button('copy2')">Copy</button>
                        <pre><code id="copy2">
from module.discord import Send_to_discord
from module.llm_agent import LLM_Agent

<var>model_name</var> = 'gemma3:12b' #사용할 모델명을 입력하세요
<var>system_prompt</var> = '당신은 유능한 비서입니다. 이용자에게 도움이 되는 답변을 제공합니다.'
<var>user_prompt</var> = '프랑스의 수도는 어디인가요?'
<var>provider</var> = 'ollama'  <span class="comment">#현재 사용가능한 provier는 "ollama", "openai","genai(gemini)"입니다</span>

<var>agent</var> = LLM_Agent(model_name, provider, api_key=None)
<var>response</var> = agent(system_prompt, user_prompt, task=None)

<var>discord</var> = Send_to_discord(base_url="your_discord_webhook_url") <span class="comment">#청크 사이즈 및 겹침 크기 설정</span>
discord.send_message(response)
                        </code></pre>
                    </div>
                </details>

                <details>
                    <summary>메시지 청크 분할 및 겹침 설정하기</summary>
                    <p>Discord로 메시지를 보낼 때, 메시지가 너무 길 경우 청크로 나누어 전송할 수 있습니다. 다음과 같이 설정할 수 있습니다.</p>
                    <div class="code-container">
                        <button class="copy_button" onclick="copy_button('copy2-1')">Copy</button>
                        <pre><code id="copy2-1">
from module.discord import Send_to_discord
from module.llm_agent import LLM_Agent

<span class="comment">#각 에이전트의 시스템 프롬프트, 사용자 프롬프트, 작업을 정의합니다.</span>
<var>multi_agent_tasks</var> = {
    "Agent 1": "도시에서 발생하는 환경 문제(대기, 수질, 쓰레기 등)를 정리하고, 가장 시급한 과제를 제시한다.",
    "Agent 2": "친환경 교통수단(대중교통, 자전거, 전기차 등)을 기반으로 지속 가능한 교통 인프라 계획을 제안한다.",
    "Agent 3": "재생에너지(태양광, 풍력, 스마트 그리드 등)를 활용하여 효율적인 에너지 공급 방안을 설계한다.",
    "Agent 4": "도시 공간 구조(공원, 주거, 상업지구 배치 등)를 최적화한다."
}
<var>multi_agent_system_prompts</var> = {
    "Agent 1": "당신은 환경 전문가입니다. 도시의 환경 문제를 분석하고, 가장 시급한 문제를 제시하세요.",
    "Agent 2": "당신은 교통 전문가입니다. 지속 가능한 교통 인프라 계획을 제안하세요.",
    "Agent 3": "당신은 에너지 전문가입니다. 재생에너지를 활용한 에너지 공급 방안을 설계하세요.",
    "Agent 4": "당신은 도시 계획 전문가입니다. 도시 공간 구조를 최적화하는 방안을 제시하세요."
}
<var>user_prompts</var> ={
    "Agent 1": "도시에서 발생하는 환경 문제를 분석하고, 가장 시급한 문제를 제시하세요.",
    "Agent 2": "지속 가능한 교통 인프라 계획을 제안하세요.",
    "Agent 3": "재생에너지를 활용한 에너지 공급 방안을 설계하세요.",
    "Agent 4": "도시 공간 구조를 최적화하는 방안을 제시하세요."
}


<var>order</var> = ["Agent 1", "Agent 2", "Agent 3", "Agent 4"]

<var>multi_agent</var> = LLM_Agent(model_name="gemini-2.5-flash", provider="genai", api_key="your_api_key") <span class="comment">#여러 에이전트를 생성합니다.</span>

<var>agent_responses</var> = {
    name: multi_agent(multi_agent_system_prompts[name], user_prompts[name], multi_agent_tasks[name])
    for name in order
}
<var>response_list</var> = [agent_responses[name] for name in order] <span class="comment">#각 에이전트의 응답을 순서대로 리스트에 저장합니다.</span>

<var>multi_agent_responses</var> = multi_agent.aggregate_responses(
    "당신은 도시 계획 전문가입니다. 지속 가능한 도시 설계 방안을 제시하세요.",
    "다음은 여러 전문가의 의견입니다. 이를 바탕으로 최종 요약 및 통합된 지속 가능한 도시 설계 방안을 제시하세요.",
    "최종 요약 및 통합된 지속 가능한 도시 설계 방안을 제시한다.",
    response_list
) <span class="comment">#모든 에이전트의 응답을 통합하여 최종 응답을 생성합니다.</span>

print("Agent 1 Response:", agent_responses["Agent 1"])
print("Agent 2 Response:", agent_responses["Agent 2"])
print("Agent 3 Response:", agent_responses["Agent 3"])
print("Agent 4 Response:", agent_responses["Agent 4"])
print("Multi-Agent Responses:", multi_agent_responses)
<var>discord</var> = Send_to_discord(base_url="your_discord_webhook_url", chunk_size=1900) <span class="comment">#청크 사이즈 및 겹침 크기 설정</span>
discord.send_message(multi_agent_responses)
                        </code></pre>
                    </div>
                    <p>위 코드에서 <code>chunk_size</code> 매개변수를 사용하여 각 메시지 청크의 최대 길이를 설정할 수 있습니다. Discord의 메시지 길이 제한을 고려하여 적절한 크기로 설정하세요. <code>overlap</code> 매개변수는 청크 간의 겹침 크기를 설정합니다. 기본값은 0이며, 필요에 따라 조정할 수 있습니다.</p>
                    <p>청크로 나누어진 메시지는 순차적으로 Discord 채널에 전송됩니다.</p>
                </details>
            </div>
        </main>
    </div>

    <!-- 부트스트랩 JS (토글/드롭다운 동작) -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.8/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>