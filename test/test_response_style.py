#!/usr/bin/env python3
"""
LLM ëª¨ë¸ë³„ ì‘ë‹µ ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸
ì¡´ëŒ“ë§ vs ë°˜ë§ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¹„êµ í…ŒìŠ¤íŠ¸
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from module.llm_agent import LLM_Agent

def test_response_styles():
    """
    ë‹¤ì–‘í•œ LLM ëª¨ë¸ì—ì„œ ì¡´ëŒ“ë§/ë°˜ë§ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
    """

    # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ë“¤
    models = [
        {"name": "gemma3n", "provider": "ollama"},
        {"name": "qwen3:8b", "provider": "ollama"},
        {"name": "gpt-oss:20b", "provider": "ollama"}
    ]

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    system_prompts = {
        "ì¡´ëŒ“ë§": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•­ìƒ ì •ì¤‘í•˜ê³  ì¡´ëŒ“ë§ë¡œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”.",
        "ë°˜ë§": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë°˜ë§ë¡œ ëŒ€ë‹µí•˜ë©´ ì•ˆë˜ìš”."
    }

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?",
        "Pythonê³¼ JavaScript ì¤‘ ì–´ë–¤ ì–¸ì–´ë¥¼ ì¶”ì²œí•˜ë‚˜ìš”?",
        "ë§›ìˆëŠ” ì €ë… ë©”ë‰´ ì¶”ì²œí•´ì£¼ì„¸ìš”"
    ]

    print("ğŸ¤– LLM ëª¨ë¸ë³„ ì‘ë‹µ ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    print("=" * 80)

    for model_info in models:
        print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ëª¨ë¸: {model_info['name']}")
        print("-" * 60)

        try:
            # LLM ì—ì´ì „íŠ¸ ìƒì„±
            agent = LLM_Agent(
                model_name=model_info['name'],
                provider=model_info['provider']
            )

            for style, system_prompt in system_prompts.items():
                print(f"\nğŸ¯ {style} í…ŒìŠ¤íŠ¸:")
                print(f"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_prompt}")
                print()

                for i, question in enumerate(test_questions, 1):
                    print(f"Q{i}: {question}")

                    try:
                        # LLMì— ì§ˆë¬¸
                        response = agent(
                            system_prompt=system_prompt,
                            user_message=question
                        )

                        print(f"A{i}: {response}")
                        print()

                    except Exception as e:
                        print(f"âŒ ì§ˆë¬¸ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        print()

                print("â”€" * 40)

        except Exception as e:
            print(f"âŒ ëª¨ë¸ {model_info['name']} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            continue

    print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("\nğŸ’¡ ê²°ê³¼ ë¶„ì„:")
    print("1. ì¡´ëŒ“ë§ ìš”ì²­ â†’ ì¡´ëŒ“ë§ë¡œ ë‹µë³€í•˜ëŠ”ì§€ í™•ì¸")
    print("2. 'ë°˜ë§í•˜ë©´ ì•ˆë˜ìš”' â†’ ì‹¤ì œë¡œ ë°˜ë§ë¡œ ë‹µë³€í•˜ëŠ”ì§€ í™•ì¸ ğŸ˜")
    print("3. ê° ëª¨ë¸ì´ ì§€ì‹œì‚¬í•­ì„ ì–´ë–»ê²Œ í•´ì„í•˜ëŠ”ì§€ ë¹„êµ")
    print("4. ëª¨ë¸ë³„ ì‘ë‹µ ìŠ¤íƒ€ì¼ê³¼ ì§€ì‹œì‚¬í•­ ì¤€ìˆ˜ë„ ê´€ì°°")

def test_single_model(model_name: str):
    """
    íŠ¹ì • ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸í•˜ëŠ” í•¨ìˆ˜
    """
    print(f"ğŸ” {model_name} ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    agent = LLM_Agent(model_name=model_name, provider="ollama")

    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    questions = [
        ("ì¡´ëŒ“ë§ ìš”ì²­", "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AIì…ë‹ˆë‹¤. ì¡´ëŒ“ë§ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.", "ì•ˆë…•í•˜ì„¸ìš”?"),
        ("ë°˜ë§ ê¸ˆì§€ ğŸ˜", "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AIì…ë‹ˆë‹¤. ë°˜ë§ë¡œ ëŒ€ë‹µí•˜ë©´ ì•ˆë˜ìš”.", "ì•ˆë…•?")
    ]

    for test_name, system_prompt, user_message in questions:
        print(f"\nğŸ“Œ {test_name} í…ŒìŠ¤íŠ¸:")
        response = agent(system_prompt=system_prompt, user_message=user_message)
        print(f"ì§ˆë¬¸: {user_message}")
        print(f"ë‹µë³€: {response}")
        print()

if __name__ == "__main__":
    print("LLM ì‘ë‹µ ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸")
    print("ì‚¬ìš©ë²•:")
    print("1. ì „ì²´ í…ŒìŠ¤íŠ¸: python test_response_style.py")
    print("2. ë‹¨ì¼ ëª¨ë¸: python test_response_style.py [ëª¨ë¸ëª…]")
    print()

    if len(sys.argv) > 1:
        # ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        model_name = sys.argv[1]
        test_single_model(model_name)
    else:
        # ì „ì²´ í…ŒìŠ¤íŠ¸
        test_response_styles()